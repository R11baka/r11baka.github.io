---
layout: post
title:  Качая 1000000 url
date:   2016-07-21 21:05:00 +0300
categories: python forfun nosql
---

Intro
=======
В 1 прекрасный день захотелось мне что-то накодить интересное и может даже полезное. Так как это только для развлечения, то времени занять это должно не много. Подумал я и решил,а скачаю я ка за разумное время (не неделю) [Alexa Top 1 million sites](http://s3.amazonaws.com/alexa-static/top-1m.csv.zip),причом если я захочу когда либо скачать больше,то система должна быть масштабируема.
### requrements ###
 - Ограниченное время
 - 1 миллион урлов
 - можно написать за день,максимум 2

После непродолжительного гугления решил я что система будет такова
Отдельно crawler-ы которые качают url-ы и ложат скачанный контент  на сервер. Отдельно сервер которые сохраняет в хранилище пары url=>urlcontent которые качают crawler. Надо также позаботится о некоем распараллеливании задач. Тут можно пойти несколькими путями, завести N crawler-ов и если у вас есть M урлов, то брать Порядковый номер URL MOD N, мы получаем остаток от деления,и отдаем полученный список crawler.
А можно пойти проще, запихнуть кудато список url,пусть crawler выбирают поочередно из хранилища url ,и качают контент.На роль такого хранилища прекрасно подходи redis ,и будем мы работать с ним как с очередью (FIFO)
На роль хранилища контена я выбрал riak (NoSql база данных на erlang)
 - 1 сервер с riak на борту
 - Тот же сервер  с redis
 - Crawler на Python 3.5 (добавили туда асинхронные штуки)

также не охота мне морочиться с настройкой этих crawler,и к всему этому надо прикрутить fabric чтоб и деплоил,и настраивал мне он все сам.

